{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running TEM simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 91\u001b[39m\n\u001b[32m     89\u001b[39m \u001b[38;5;66;03m# print(sim)\u001b[39;00m\n\u001b[32m     90\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRunning sim...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m \u001b[43msim\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_sim\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSim finished.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/TEM/NeuralPlayground/neuralplayground/backend/simulation_manager.py:319\u001b[39m, in \u001b[36mSingleSim.run_sim\u001b[39m\u001b[34m(self, save_path)\u001b[39m\n\u001b[32m    317\u001b[39m \u001b[38;5;66;03m# Training loop\u001b[39;00m\n\u001b[32m    318\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m---> Training loop\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m trained_agent, trained_env, training_hist = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_loop_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    321\u001b[39m \u001b[38;5;66;03m# Saving models\u001b[39;00m\n\u001b[32m    322\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m---> Saving models\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/TEM/NeuralPlayground/neuralplayground/backend/training_loops.py:105\u001b[39m, in \u001b[36mtem_training_loop\u001b[39m\u001b[34m(agent, env, n_episode, params)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_episode):\n\u001b[32m    104\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m agent.n_walk < params[\u001b[33m\"\u001b[39m\u001b[33mn_rollout\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m         actions = \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatch_act\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    106\u001b[39m         obs, state, reward = env.step(actions, normalize_step=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    107\u001b[39m     agent.update()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/TEM/NeuralPlayground/neuralplayground/agents/whittington_2020.py:184\u001b[39m, in \u001b[36mWhittington2020.batch_act\u001b[39m\u001b[34m(self, observations, policy_func)\u001b[39m\n\u001b[32m    181\u001b[39m     \u001b[38;5;28mself\u001b[39m.n_walk += \u001b[32m1\u001b[39m\n\u001b[32m    183\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.use_behavioural_data:\n\u001b[32m--> \u001b[39m\u001b[32m184\u001b[39m     locations = [\u001b[43menv\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m env \u001b[38;5;129;01min\u001b[39;00m observations]\n\u001b[32m    185\u001b[39m     all_allowed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    186\u001b[39m     new_actions = []\n",
      "\u001b[31mIndexError\u001b[39m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This file runs a training simulation for the Whittington et al. 2020 agent, the Tolman-Eichenbaum Machine (TEM).\n",
    "The TEM is a model of the hippocampus that learns to navigate a series of environments and solve a series of tasks.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from neuralplayground.agents.whittington_2020 import Whittington2020 \n",
    "## imports the TEM model (\"agent\")\n",
    "from neuralplayground.agents.whittington_2020_extras import whittington_2020_parameters as parameters \n",
    "## provides all the parameters/settings for the model\n",
    "from neuralplayground.arenas import Simple2D #changed \n",
    "## defines area/world. batch env: , simple2D:\n",
    "from neuralplayground.backend import SingleSim, tem_training_loop\n",
    "## simulation manager and specific training procedure for model/agent\n",
    "from neuralplayground.experiments import Sargolini2006Data\n",
    "## import experimental data (not really used in this code)\n",
    "\n",
    "simulation_id = \"TEM_arena_sim\"\n",
    "save_path = os.path.join(os.getcwd(), \"results_sim\")\n",
    "# save_path = os.path.join(os.getcwd(), \"examples\", \"agent_examples\", \"trained_results\")\n",
    "agent_class = Whittington2020\n",
    "env_class = Simple2D\n",
    "training_loop = tem_training_loop\n",
    "## defines the core components of the simulation/model\n",
    "\n",
    "\n",
    "arena_x_limits = (0, 7)\n",
    "arena_y_limits = (0, 7)\n",
    "\n",
    "#room_widths = [int(arena_x_limits[0][1] - arena_x_limits[0][0])]\n",
    "#room_depths = [int(arena_y_limits[0][1] - arena_y_limits[0][0])]\n",
    "\n",
    "#calculating width and depth for a single arena\n",
    "room_widths = [arena_x_limits[1] - arena_x_limits[0]]\n",
    "room_depths = [arena_y_limits[1] - arena_y_limits[0]]\n",
    "\n",
    "#room_widths = [int(np.diff(arena_x_limits)[i]) for i in range(len(arena_x_limits))]\n",
    "#room_depths = [int(np.diff(arena_y_limits)[i]) for i in range(len(arena_y_limits))]\n",
    "\n",
    "#discrete_env_params = {\n",
    "    #\"environment_name\": \"DiscreteObject\",\n",
    "    #\"state_density\": 1,\n",
    "    #\"n_objects\": params[\"n_x\"],\n",
    "    #\"agent_step_size\": 1,\n",
    "    #\"use_behavioural_data\": False,\n",
    "    #\"data_path\": None,\n",
    "    #\"experiment_class\": Sargolini2006Data,\n",
    "#}\n",
    "\n",
    "env_params = {\n",
    "    \"environment_name\": \"Simple2D_1\",\n",
    "    \"arena_x_limits\": arena_x_limits,\n",
    "    \"arena_y_limits\": arena_y_limits,\n",
    "}\n",
    "\n",
    "params = parameters.parameters()\n",
    "full_agent_params = params.copy()\n",
    "full_agent_params[\"n_x\"] = room_widths[0] * room_depths[0]\n",
    "full_agent_params[\"batch_size\"] = 1\n",
    "   \n",
    "\n",
    "agent_params = {\n",
    "    \"model_name\": \"Whittington2020\",\n",
    "    \"params\": full_agent_params,\n",
    "    \"batch_size\": 1,\n",
    "    \"room_widths\": room_widths,\n",
    "    \"room_depths\": room_depths,   \n",
    "    \"state_densities\": [1], #not sure \n",
    "    \"use_behavioural_data\": False,\n",
    "}\n",
    "\n",
    "\n",
    "# Full model training consists of 20000 episodes\n",
    "training_loop_params = {\"n_episode\": 10, \"params\": full_agent_params}\n",
    "\n",
    "sim = SingleSim(\n",
    "    simulation_id=simulation_id,\n",
    "    agent_class=agent_class,\n",
    "    agent_params=agent_params,\n",
    "    env_class=env_class,\n",
    "    env_params=env_params,\n",
    "    training_loop=training_loop,\n",
    "    training_loop_params=training_loop_params,\n",
    ")\n",
    "\n",
    "# print(sim)\n",
    "print(\"Running sim...\")\n",
    "sim.run_sim(save_path)\n",
    "print(\"Sim finished.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting TEM Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SystemExit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from neuralplayground.comparison import GridScorer\n",
    "from neuralplayground.plotting import PlotSim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulation_id = \"TEM_custom_plot_sim\"\n",
    "from neuralplayground.saved_models import fetch_model_path\n",
    "\n",
    "#save_path = fetch_model_path(\"whittington_2020_in_discritized_objects\")\n",
    "print(save_path)\n",
    "plotting_loop_params = {\"n_episode\": 5000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dict = pd.read_pickle(os.path.join(os.getcwd(), save_path, \"params.dict\"))\n",
    "model_weights = pd.read_pickle(os.path.join(save_path, \"agent\"))\n",
    "model_spec = importlib.util.spec_from_file_location(\"model\", save_path + \"/whittington_2020_model.py\")\n",
    "model = importlib.util.module_from_spec(model_spec)\n",
    "model_spec.loader.exec_module(model)\n",
    "params = pd.read_pickle(os.path.join(save_path, \"agent_hyper\"))\n",
    "tem = model.Model(params)\n",
    "tem.load_state_dict(model_weights)\n",
    "tem.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = PlotSim(\n",
    "    simulation_id=simulation_id,\n",
    "    agent_class=training_dict[\"agent_class\"],\n",
    "    agent_params=training_dict[\"agent_params\"],\n",
    "    env_class=training_dict[\"env_class\"],\n",
    "    env_params=training_dict[\"env_params\"],\n",
    "    plotting_loop_params=plotting_loop_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_agent, trained_env = sim.plot_sim(save_path, n_walks=1000, random_state=False, custom_state=[0.5,0.5])\n",
    "trained_env.plot_trajectories();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(save_path, \"NPG_environments.pkl\"), \"rb\") as f:\n",
    "    environments = pickle.load(f)\n",
    "with open(os.path.join(save_path, \"NPG_model_input.pkl\"), \"rb\") as f:\n",
    "    model_input = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dict[\"params\"] = training_dict[\"agent_params\"]\n",
    "del training_dict[\"agent_params\"]\n",
    "agent = training_dict[\"agent_class\"](**training_dict[\"params\"])\n",
    "agent.plot_run(tem, model_input, environments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.plot_rate_map(rate_map_type='g')\n",
    "agent.plot_rate_map(rate_map_type='p');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate_map_mat = agent.get_rate_map_matrix(agent.g_rates,1,2)\n",
    "GridScorer = GridScorer(rate_map_mat.shape[0])\n",
    "GridScorer.plot_grid_score(r_out_im = rate_map_mat, plot= True)\n",
    "score = GridScorer.get_scores(rate_map_mat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TEM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
